import os
import torch
import numpy as np
import sounddevice as sd
import queue
import threading
import time
import signal
import wave
import pyaudio
import tkinter as tk
from tkinter import ttk, scrolledtext
from faster_whisper import WhisperModel
import soundfile as sf
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, SpeechT5Processor, SpeechT5ForTextToSpeech
from threading import Event

# Global settings
SAMPLE_RATE = 16000
CHUNK_DURATION = 0.2  # Process in very small chunks (200ms) for near real-time results
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION)
CHANNELS = 1
FORMAT = pyaudio.paFloat32
DTYPE = np.float32

# Translation settings
SRC_LANG = "en"
TGT_LANG = "fr"  # Change to desired target language

# Model settings
WHISPER_MODEL = "small"  # Using smaller model for faster processing
M2M_MODEL = "facebook/m2m100_418M"

# Queue for audio chunks
audio_queue = queue.Queue()
# Queue for translated text (whole sentences)
translated_text_queue = queue.Queue()
# Queue for TTS audio
tts_queue = queue.Queue()

# Control flags
is_running = False
models_loaded = Event()

# For tracking translation
current_transcription = ""
current_translation = ""
last_translation_update = 0
translation_debounce = 0.5  # Wait at least 0.5s between translations to prevent repeated words

# For handling audio
input_audio_buffer = np.array([], dtype=DTYPE)
input_audio_start_time = 0
translation_delay = 0  # To track system latency

# GUI elements
root = None
translation_display = None
original_text_display = None
stats_label = None

# For speech detection
silence_threshold = 0.01
silence_duration = 0.5
is_speaking = False
last_speech_time = 0

# Models
whisper_model = None
m2m_tokenizer = None
m2m_model = None
processor = None
tts_model = None
speaker_embeddings = None

def load_models():
    """Load all required models with optimizations for speed."""
    global whisper_model, m2m_tokenizer, m2m_model, processor, tts_model, speaker_embeddings
    
    try:
        print("Loading Whisper ASR model (small version for speed)...")
        # Use compute_type="int8" for even faster processing on CPU
        whisper_model = WhisperModel(
            WHISPER_MODEL, 
            device="cuda" if torch.cuda.is_available() else "cpu", 
            compute_type="float16" if torch.cuda.is_available() else "int8"
        )
        
        print("Loading M2M-100 translation model...")
        # Use torch.float16 for faster processing
        m2m_tokenizer = M2M100Tokenizer.from_pretrained(M2M_MODEL)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        m2m_model = M2M100ForConditionalGeneration.from_pretrained(M2M_MODEL).to(device)
        if torch.cuda.is_available():
            m2m_model = m2m_model.half()  # Use half precision on GPU
        m2m_tokenizer.src_lang = SRC_LANG
        
        print("Loading SpeechT5 TTS model...")
        processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
        tts_model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts").to(device)
        if torch.cuda.is_available():
            tts_model = tts_model.half()  # Use half precision on GPU
        # Create speaker embeddings for voice characteristics
        speaker_embeddings = torch.randn(1, 512).to(device)
        if torch.cuda.is_available():
            speaker_embeddings = speaker_embeddings.half()
        
        models_loaded.set()
        print("All models loaded successfully.")
    except Exception as e:
        print(f"Error loading models: {e}")
        # Set the event to prevent hanging even if model loading fails
        models_loaded.set()
        raise

def detect_speech(audio_data, threshold=0.01):
    """Detect if the audio contains speech based on amplitude."""
    return np.max(np.abs(audio_data)) > threshold

def capture_audio_stream():
    """Capture audio in small chunks for real-time processing."""
    global input_audio_buffer, input_audio_start_time, is_running, is_speaking, last_speech_time
    
    p = None
    stream = None
    
    try:
        p = pyaudio.PyAudio()
        
        def callback(in_data, frame_count, time_info, status):
            global input_audio_buffer, input_audio_start_time, is_speaking, last_speech_time
            
            if is_running:
                audio_data = np.frombuffer(in_data, dtype=DTYPE)
                
                # Check if we're speaking
                current_time = time.time()
                if detect_speech(audio_data, silence_threshold):
                    is_speaking = True
                    last_speech_time = current_time
                elif is_speaking and (current_time - last_speech_time > silence_duration):
                    is_speaking = False
                
                # Only add to queue if speaking or recently spoke
                if is_speaking or (current_time - last_speech_time < silence_duration * 2):
                    # Ensure queue is not getting too large
                    if audio_queue.qsize() < 50:  # Reduced from 100 to 50
                        audio_queue.put(audio_data)
                
                # Store for processing
                if input_audio_buffer.size == 0:
                    input_audio_start_time = time.time()
                input_audio_buffer = np.append(input_audio_buffer, audio_data)
                
                # Limit buffer to 3 seconds to prevent memory issues (reduced from 5)
                max_buffer = SAMPLE_RATE * 3
                if input_audio_buffer.size > max_buffer:
                    input_audio_buffer = input_audio_buffer[-max_buffer:]
            
            return (in_data, pyaudio.paContinue)
        
        stream = p.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=SAMPLE_RATE,
            input=True,
            output=False,
            frames_per_buffer=CHUNK_SIZE,
            stream_callback=callback
        )
        
        print("Audio capture started")
        stream.start_stream()
        
        # Keep thread alive
        while is_running:
            time.sleep(0.1)
            
    except Exception as e:
        print(f"Error in audio capture: {e}")
    finally:
        # Clean up resources
        if stream is not None and stream.is_active():
            try:
                stream.stop_stream()
                stream.close()
            except:
                pass
                
        if p is not None:
            try:
                p.terminate()
            except:
                pass
                
        print("Audio capture stopped")

def process_audio_realtime():
    """Process audio chunks continuously with improved sentence detection."""
    # Wait for models to be loaded
    if not models_loaded.wait(timeout=60):
        print("Timeout waiting for models to load. Exiting audio processing thread.")
        return
    
    global current_transcription, current_translation, translation_delay, is_running, last_translation_update
    
    # Buffer to accumulate audio for processing
    processing_buffer = np.array([], dtype=DTYPE)
    accumulated_duration = 0
    min_process_duration = 0.5  # Process at least 0.5 seconds of audio at a time
    max_process_duration = 1.5  # Reduced from 2.0s to 1.5s for lower latency
    
    # For sentence detection
    sentence_end_chars = ['.', '?', '!']
    temp_file = "temp_audio_processing.wav"
    last_segment_end = ""
    
    try:
        while is_running or not audio_queue.empty():
            # Get audio chunk
            try:
                chunk = audio_queue.get(timeout=0.1)
                audio_queue.task_done()
            except queue.Empty:
                # If no new audio for a while and we have some buffered audio, force processing
                if processing_buffer.size > 0 and time.time() - last_translation_update > 1.0:
                    pass  # Will proceed to processing below
                else:
                    continue
            
            # Add chunk to processing buffer
            if 'chunk' in locals():
                processing_buffer = np.append(processing_buffer, chunk)
                chunk_duration = len(chunk) / SAMPLE_RATE
                accumulated_duration += chunk_duration
            
            # Process if we have enough audio or forced processing
            process_now = accumulated_duration >= min_process_duration or \
                         (processing_buffer.size > 0 and time.time() - last_translation_update > 1.0)
            
            if process_now:
                # Skip processing if buffer is too quiet (likely silence)
                if np.max(np.abs(processing_buffer)) < silence_threshold and not is_speaking:
                    processing_buffer = np.array([], dtype=DTYPE)
                    accumulated_duration = 0
                    continue
                
                # Save buffer to temporary file
                try:
                    sf.write(temp_file, processing_buffer, SAMPLE_RATE)
                except Exception as e:
                    print(f"Error writing temp audio file: {e}")
                    continue
                
                processing_start = time.time()
                
                try:
                    if not os.path.exists(temp_file) or os.path.getsize(temp_file) == 0:
                        continue
                    
                    # Transcribe with improved settings
                    result = whisper_model.transcribe(
                        temp_file, 
                        beam_size=1,  # Reduced beam size for speed
                        language=SRC_LANG,
                        vad_filter=True,  # Add voice activity detection
                        condition_on_previous_text=True  # Improve context
                    )
                    
                    # Get transcribed text
                    transcribed_text = ""
                    for segment in result[0]:
                        transcribed_text += segment.text + " "
                    
                    transcribed_text = transcribed_text.strip()
                    
                    # Avoid processing if no meaningful text was detected
                    if not transcribed_text or transcribed_text.isspace():
                        continue
                        
                    # Update the current transcription
                    current_transcription = transcribed_text
                    
                    # Throttle translation updates to prevent repeats
                    current_time = time.time()
                    if current_time - last_translation_update >= translation_debounce:
                        # Translate
                        m2m_tokenizer.src_lang = SRC_LANG
                        inputs = m2m_tokenizer(transcribed_text, return_tensors="pt").to(
                            "cuda" if torch.cuda.is_available() else "cpu"
                        )
                        m2m_tokenizer.tgt_lang = TGT_LANG
                        
                        with torch.no_grad():
                            translated_tokens = m2m_model.generate(
                                **inputs, 
                                forced_bos_token_id=m2m_tokenizer.get_lang_id(TGT_LANG),
                                max_length=150,
                                num_beams=2  # Reduced beam size for speed
                            )
                        
                        translated_text = m2m_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)
                        
                        # Record translation delay
                        translation_delay = time.time() - processing_start
                        last_translation_update = current_time
                        
                        # Update tracking variables
                        current_translation = translated_text
                        
                        # Send for display and TTS - only if translation changed significantly
                        if translated_text_queue.qsize() < 10:  # Reduced queue size limit
                            translated_text_queue.put((transcribed_text, translated_text))


                            
                            # Only synthesize speech for complete sentences or phrases
                            if any(char in translated_text for char in sentence_end_chars) or \
                               len(translated_text.split()) >= 3:  # At least 3 words
                                if tts_queue.qsize() < 5:  # Reduced queue size limit
                                    tts_queue.put(translated_text)
                
                except Exception as e:
                    print(f"Error in transcription/translation: {e}")
                    
                finally:
                    # Reset buffer if we've accumulated too much audio
                    if accumulated_duration >= max_process_duration:
                        # Keep a small overlap for context
                        overlap_duration = 0.2
                        overlap_samples = int(overlap_duration * SAMPLE_RATE)
                        if len(processing_buffer) > overlap_samples:
                            processing_buffer = processing_buffer[-overlap_samples:]
                            accumulated_duration = overlap_duration
                        else:
                            processing_buffer = np.array([], dtype=DTYPE)
                            accumulated_duration = 0
                    
                    # Clean up temp file
                    try:
                        if os.path.exists(temp_file):
                            os.remove(temp_file)
                    except:
                        pass
                    
    except Exception as e:
        print(f"Error in audio processing thread: {e}")
    finally:
        # Clean up temp file on exit
        try:
            if os.path.exists(temp_file):
                os.remove(temp_file)
        except:
            pass
            
        print("Audio processing thread completed")

def text_to_speech_realtime():
    """Generate and play speech with improved handling."""
    # Wait for models to be loaded
    if not models_loaded.wait(timeout=60):
        print("Timeout waiting for models to load. Exiting TTS thread.")
        return
    
    global is_running
    
    p = None
    stream = None
    
    try:
        p = pyaudio.PyAudio()
        # Setup audio output stream
        stream = p.open(
            format=pyaudio.paFloat32,
            channels=1,
            rate=24000,  # SpeechT5 output rate
            output=True,
            frames_per_buffer=1024
        )
        
        # Track last processed text to avoid duplicates
        last_tts_text = ""
        
        while is_running or not tts_queue.empty():
            try:
                # Get translation with timeout
                translated_text = tts_queue.get(timeout=0.1)
                tts_queue.task_done()
            except queue.Empty:
                continue
            
            # Skip empty text or duplicates
            if not translated_text.strip() or translated_text == last_tts_text:
                continue
                
            last_tts_text = translated_text
            
            # Generate speech
            try:
                with torch.no_grad():
                    inputs = processor(text=translated_text, return_tensors="pt").to(
                        "cuda" if torch.cuda.is_available() else "cpu"
                    )
                    speech = tts_model.generate_speech(
                        inputs["input_ids"], 
                        speaker_embeddings
                    ).cpu().numpy()
                
                # Play if stream is active
                if stream.is_active():
                    stream.write(speech.astype(np.float32).tobytes())
                
            except Exception as e:
                print(f"Error in TTS generation: {e}")
                
    except Exception as e:
        print(f"Error in TTS thread: {e}")
    finally:
        # Clean up resources
        if stream is not None:
            try:
                stream.stop_stream()
                stream.close()
            except:
                pass
                
        if p is not None:
            try:
                p.terminate()
            except:
                pass
                
        print("TTS thread completed")

def update_gui():
    """Update the GUI with translations using improved handling."""
    global translation_display, original_text_display, stats_label, translation_delay, root, is_running
    
    if not is_running or root is None or not root.winfo_exists():
        return
    
    try:
        # Check for new translations
        updated = False
        while not translated_text_queue.empty():
            try:
                original, translation = translated_text_queue.get_nowait()
                translated_text_queue.task_done()
                updated = True
                
                # Update the translation display
                if translation_display is not None:
                    translation_display.config(state=tk.NORMAL)
                    translation_display.delete(1.0, tk.END)
                    translation_display.insert(tk.END, translation)
                    translation_display.see(tk.END)
                    translation_display.config(state=tk.DISABLED)
                
                # Update the original text display
                if original_text_display is not None:
                    original_text_display.config(state=tk.NORMAL)
                    original_text_display.delete(1.0, tk.END)
                    original_text_display.insert(tk.END, original)
                    original_text_display.see(tk.END)
                    original_text_display.config(state=tk.DISABLED)
                    
            except queue.Empty:
                break
        
        # Update statistics
        if stats_label is not None and translation_delay > 0:
            speaking_status = "Speaking detected" if is_speaking else "No speech detected"
            stats_label.config(text=f"Translation delay: {translation_delay:.2f}s | {speaking_status}")
        
        # Schedule next update
        if root is not None and root.winfo_exists():
            root.after(50, update_gui)  # Faster refresh rate (50ms)
            
    except tk.TclError as e:
        # Tkinter errors usually occur when the window is being destroyed
        pass
    except Exception as e:
        print(f"Error updating GUI: {e}")

def create_gui():
    """Create an improved GUI for displaying translations."""
    global root, translation_display, original_text_display, stats_label, is_running
    
    try:
        root = tk.Tk()
        root.title("Realtime Speech Translation")
        root.geometry("800x500")  # Taller window for more content
        
        # Handle window close event
        def on_closing():
            global is_running
            is_running = False
            try:
                root.destroy()
            except:
                pass
        
        root.protocol("WM_DELETE_WINDOW", on_closing)
        
        frame = ttk.Frame(root, padding="10")
        frame.pack(fill=tk.BOTH, expand=True)
        
        # Source language label with styling
        header_frame = ttk.Frame(frame)
        header_frame.pack(fill=tk.X, pady=5)
        
        ttk.Label(
            header_frame, 
            text=f"Realtime Speech Translation", 
            font=("Arial", 16, "bold")
        ).pack(side=tk.LEFT, pady=5)
        
        # Original speech display section
        original_frame = ttk.LabelFrame(frame, text=f"Original ({SRC_LANG})", padding=5)
        original_frame.pack(fill=tk.BOTH, expand=True, pady=5)
        
        original_text_display = scrolledtext.ScrolledText(
            original_frame, 
            wrap=tk.WORD, 
            height=5, 
            font=("Arial", 12)
        )
        original_text_display.pack(fill=tk.BOTH, expand=True)
        original_text_display.config(state=tk.DISABLED)
        
        # Translation display section
        translation_frame = ttk.LabelFrame(frame, text=f"Translation ({TGT_LANG})", padding=5)
        translation_frame.pack(fill=tk.BOTH, expand=True, pady=5)
        
        translation_display = scrolledtext.ScrolledText(
            translation_frame, 
            wrap=tk.WORD, 
            height=10, 
            font=("Arial", 14, "bold")
        )
        translation_display.pack(fill=tk.BOTH, expand=True)
        translation_display.config(state=tk.DISABLED)
        
        # Statistics display
        stats_frame = ttk.Frame(frame)
        stats_frame.pack(fill=tk.X, pady=5)
        
        stats_label = ttk.Label(
            stats_frame, 
            text="System initializing...", 
            font=("Arial", 10)
        )
        stats_label.pack(side=tk.LEFT, pady=5)
        
        # Controls
        control_frame = ttk.Frame(frame)
        control_frame.pack(fill=tk.X, pady=10)
        
        def stop_application():
            global is_running
            is_running = False
            try:
                root.destroy()
            except:
                pass
        
        ttk.Button(
            control_frame, 
            text="Stop", 
            command=stop_application,
            style="Accent.TButton"
        ).pack(side=tk.RIGHT)
        
        # Create custom button style
        style = ttk.Style()
        style.configure("Accent.TButton", font=("Arial", 11))
        
        # Start GUI update loop with improved refresh rate
        root.after(50, update_gui)
        
        return root
    except Exception as e:
        print(f"Error creating GUI: {e}")
        is_running = False
        return None

def signal_handler(sig, frame):
    """Handle interruption signals."""
    global is_running, root
    print("\nShutting down, please wait...")
    is_running = False
    
    if root is not None:
        try:
            if root.winfo_exists():
                root.destroy()
        except:
            pass

def main():
    """Main function for realtime translation system."""
    global is_running, silence_threshold
    
    # Register signal handlers
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        is_running = True
        
        # Parse command line arguments (if any)
        import argparse
        parser = argparse.ArgumentParser(description='Realtime Speech Translation')
        parser.add_argument('--src', type=str, default='en', help='Source language code')
        parser.add_argument('--tgt', type=str, default='fr', help='Target language code')
        parser.add_argument('--silence', type=float, default=0.01, help='Silence threshold (0.005-0.05)')
        args = parser.parse_args()
        
        # Update global settings if provided
        global SRC_LANG, TGT_LANG
        SRC_LANG = args.src
        TGT_LANG = args.tgt
        silence_threshold = args.silence
        
        # Create and start the model loading thread
        model_thread = threading.Thread(target=load_models)
        model_thread.daemon = True
        model_thread.start()
        
        print("Loading models, please wait...")
        
        # Start worker threads
        audio_thread = threading.Thread(target=capture_audio_stream)
        processing_thread = threading.Thread(target=process_audio_realtime)
        tts_thread = threading.Thread(target=text_to_speech_realtime)
        
        audio_thread.daemon = True
        processing_thread.daemon = True
        tts_thread.daemon = True
        
        audio_thread.start()
        processing_thread.start()
        tts_thread.start()
        
        # Wait for models to load before showing GUI (with timeout)
        if not models_loaded.wait(timeout=120):  # 2 minute timeout
            print("Warning: Timeout waiting for models to load. Proceeding anyway.")
        
        print("\n" + "="*50)
        print("IMPROVED Realtime Speech Translation System Ready")
        print(f"Source language: {SRC_LANG}, Target language: {TGT_LANG}")
        print(f"Silence threshold: {silence_threshold}")
        print("="*50 + "\n")
        
        # Create and run GUI (this blocks until GUI is closed)
        gui_root = create_gui()
        if gui_root:
            gui_root.mainloop()
        
    except Exception as e:
        print(f"Error in main thread: {e}")
    finally:
        # Signal threads to stop
        is_running = False
        
        # Give threads a moment to clean up
        time.sleep(1)
        
        print("System shutdown complete.")

if __name__ == "__main__":
    main()
